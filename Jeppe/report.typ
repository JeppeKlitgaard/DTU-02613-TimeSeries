#import "@preview/physica:0.9.4": super-T-as-transpose

#set heading(numbering: "A.1.1")
#set math.equation(numbering: "(1)", supplement: [Eq.])
#show: super-T-as-transpose
#set table.header()

#let below(x, b) = {
  math.scripts(math.attach(math.limits(x), b: b))
}

#let underbar(x) = {
  below(x, math.macron)
}

#let mm(x) = {
  let out = math.upright(x)
  out = math.bold(out)
  out = underbar(out)
  out = underbar(out)

  out
}

#let vv(x) = {
  let out = math.upright(x)
  out = math.bold(out)
  out = underbar(out)

  out
}

#let mref(key) = {
  let lbl = label(key)
  math.text([(#ref(lbl, supplement: none))])
}

// Math defines
#let Œ∏ = $vv(Œ∏)$
#let Œ∏_pred = $hat(#Œ∏)$
#let X = $mm(X)$
#let X_pred = $hat(#X)$
#let y = $vv(y)$
#let y_pred = $hat(#y)$
#let Œµ = $vv(œµ)$

= Assignment 1

== Plot Data

=== Construct time variable

=== Plotting observations

#image("output/plot_observations.png")

== Linear Trend Model

We are given the General Linear Model (GLM) in sloppy notation:
$
  Y_t = Œ∏_i + Œ∏_2 ‚ãÖ x_t + œµ_t
$ <GLM>

=== Matrix Form

We rewrite @GLM as a matrix:

$
  #y = #X #Œ∏ + #Œµ
$ <GLM_matrix>

Where $#X$ denotes the _design matrix_, $#Œ∏$ the _parameter vector_, and $#Œµ$ represents a stochastic noise term, $#Œµ ‚àº ùí©(0, œÉ^2)$.

In our case, the _design matrix_ is constructed with an intercept $Œ∏_1$ and a single trend parameter $Œ∏_2$:
$
  #X = mat(delim: "[",
    1, x_1;
    1, x_2;
    ‚ãÆ, ‚ãÆ;
    1, x_n
  )
$

Using the first 3 data points of the given data, we can construct the following
model:

$
  #y &= #X #Œ∏ + #Œµ \
  mat(delim: "[",
    y_1;
    y_2;
    y_3
  ) &= mat(delim: "[",
    1, x_1;
    1, x_2;
    1, x_3;
  ) mat(delim: "[",
    Œ∏_1;
    Œ∏_2;
  ) + mat(delim: "[",
    œµ_1;
    œµ_2;
    œµ_3;
  ) \
  mat(delim: "[",
    2930483;
    2934044;
    2941422;
  ) &= mat(delim: "[",
    1, 2018.000;
    1, 2018.083;
    1, 2018.167;
  ) mat(delim: "[",
    Œ∏_1;
    Œ∏_2;
  ) + mat(delim: "[",
    œµ_1;
    œµ_2;
    œµ_3;
  )
$

=== Parameter Estimation

We can estimate the parameters $#Œ∏$ by deriving the _normal equations_:

$
  mref("GLM_matrix") quad ‚áí quad
  #X^T #y = #X^T #X #Œ∏_pred quad ‚áí quad
  #Œ∏_pred = (#X^T #X)^(-1) #X^T #y
$ <theta_estimation>

Where $#X$ has assumed to be invertible.

We additionally consider the standard errors of the parameter estimates, $#Œ∏_pred$.

Under the assumption that the observations are described by @GLM, that is, the data is drawn from a _Simple Linear Model_ overlaid with a stochastic (i.i.d) noise term,
we find that the residuals of the mdoel are exactly the noise term, $#Œµ$.

$
  #Œµ = #y - #y_pred
$

Where

$
#y_pred = #X #Œ∏_pred
$ <y_pred>


For the benefit of the reader,
we reproduce the relationship between the residuals and the covariance matrix,
$mm(Œ£)$ @Madsen_2008[p.~36-37]:

From @theta_estimation, we have:
$
  ùîº[#Œ∏_pred]
  &= ùîº[(#X^T #X)^(-1) #X^T #y] \
  &= ùîº[(#X^T #X)^(-1) #X^T (#X #Œ∏ + #Œµ)]    wide wide && mref("GLM_matrix") \
  &= ùîº[(#X^T #X)^(-1) (#X^T #X) #Œ∏]    wide wide && #Œµ ‚àº vv(ùí©)(0, œÉ^2) \
  &= #Œ∏
$

We consider the following:
$
  #Œ∏_pred - ùîº[#Œ∏_pred]
  &= (#X^T #X)^(-1) #X^T #y - #Œ∏ \
  &= (#X^T #X)^(-1) #X^T (#X #Œ∏ + #Œµ) - #Œ∏ \
  &= (#X^T #X)^(-1) #X^T #Œµ
$

We can now evaluate the covariance of the predicted parameters, #Œ∏_pred:
$
  "Cov"[#Œ∏_pred]
  &= ùîº[(#Œ∏_pred - ùîº[#Œ∏_pred]) (#Œ∏_pred - ùîº[#Œ∏_pred])^T]\
  &= ùîº[((#X^T #X)^(-1) #X^T #Œµ) ((#X^T #X)^(-1) #X^T #Œµ)^T]\
  &= ùîº[(#X^T #X)^(-1) #X^T #Œµ #Œµ^T #X ((#X^T #X)^(-1))^T]\
  &= (#X^T #X)^(-1) #X^T ùîº[#Œµ #Œµ^T] #X ((#X^T #X)^T)^(-1)\
  &= (#X^T #X)^(-1) #X^T "Var"[#Œµ #Œµ^T] #X ((#X^T #X)^T)^(-1)\
  &= (#X^T #X)^(-1) #X^T œÉ^2 #X (#X^T #X)^(-1)    wide wide wide wide && #Œµ ‚àº vv(ùí©)(0, œÉ^2) \
  &= œÉ^2 (#X^T #X)^(-1) \
$

Where we understand the variances of the predicted variables to be
the diagonal elements of the covariance matrix:

$
  "Var"[#Œ∏_pred]
  &= "diag"("Cov"[#Œ∏_pred])
$

Which gives the following standard errors for the parameter estimates:

$
  œÉ_(hat(Œ∏_i)) = sqrt("Var"[hat(Œ∏_i)]) = sqrt(œÉ^2 (#X^T #X)^(-1)_(i)) wide wide i ‚àà {1, 2}
$

Computing these for the entire training dataset gives:

$
  hat(Œ∏_1) &= (-110360 ¬± 60) √ó 10^3\
  hat(Œ∏_2) &= (3593.6 ¬± 1.8) √ó 10^3\
$

Using these predicted parameters,
we are able to produce an estimate of the vehicle registrations for the training dataset
using the General Linear Model as seen in @fig:plot_observations_predicted.

#figure(
  image("output/plot_observations_predicted.png"),
  caption: [Estimation of vehicle registrations using the General Linear Model.]
) <fig:plot_observations_predicted>

=== Prediction

We now wish to predict future vehicle registrations using our simple model.
From @y_pred, we see that doing so simply requires the construction of an appropriate
design matrix, #X_pred.
This may be constructed simply as:

$
  #X_pred _i = mat(delim: "[",
    1, 2024 + (i-1)/12
  ) wide wide i ‚àà {1, 2, ‚Ä¶, 12}
$

Where $i$ indexes the rows of the design matrix.

Carrying out the forecasting as described by @y_pred
along with appropriate estimation of the confidence interval of our prediction,
we obtain @table:forecast_2_3.
It should be noted that the estimation of the confidence interval valid only
in the case where the observations are described by the General Linear Model.

#let forecast_2_3 = csv("output/forecast.csv")
#figure(
  table(
    columns: 4,
    table.header(..forecast_2_3.remove(0).map(strong)),
    ..forecast_2_3.flatten()
  ),
  caption: [12 month forecast of vehicle registrations in Denmark.]
) <table:forecast_2_3>

=== Plot of Forecast

We can now present the forecasted data in @table:forecast_2_3
along with the training, test, and prediction data sets

#figure(
  image("output/plot_observations_forecast.png"),
  caption: [12 month forecast of vehicle registrations in Denmark.]
)

=== Commentary on Forecast <sec:2_5_commentary>

We find that the prediction is a relatively poor match against the test data,
from which we understand that a Simple Linear Model is likely not an appropriate
model for the data.

In particular, we observe significant local deviations from the model,
which can be understood by considering the modelling domain.
It is reasonable to assume that the number of registered vehicles will
be influenced by market conditions,
such as government subsidies, registration fees, and taxation schemes.
Additionally we would expect supply chain disruptions to have strongly influence
the contemporary pricing and availability of vehicles.

A better model would likely incorporate these factors.
A model that incorporates locality without apriori domain knowledge could be
a _Weighted Least Squares_ model with local weights.

=== Residual Analysis

In order to substantiate @sec:2_5_commentary,
we perform a residual analysis on the prediction and forecasting data.

Recalling the assumptions of our model,
we expect the residuals to be normally distributed around zero:

$
vv(r) = #y - #y_pred ‚àº ùí©(0, œÉ^2)
$

It is readily apparent in @fig:plot_2_6_forecast_residuals that the residuals
are not normally distributed, nor do they average to zero:

$
  ùîº[vv(r)] = -8739
$

As such, we conclude that the model is not appropriate to describe the data.

#figure(
  image("output/plot_2_6_forecast_residuals.png"),
  caption: [
    Residual analysis on prediction and forecasting of total vehicle registrations in Denmark.
    Dashed black line delineates the transition from prediction to forecasting.]
) <fig:plot_2_6_forecast_residuals>

#bibliography("report.bib")
